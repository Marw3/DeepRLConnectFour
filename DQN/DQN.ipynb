{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd2cee5",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "---\n",
    "\n",
    "> Internship neural networks\n",
    ">\n",
    "> Group 4: Reinforcement learning\n",
    ">\n",
    "> Deadline 28.02.23 23:59\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250b14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30c99b",
   "metadata": {},
   "source": [
    "### Define network\n",
    "\n",
    "This is the network used to calculate the q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c9a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''\n",
    "    This is the feed forward network to predict the q values for the given states.\n",
    "\n",
    "    n_actions (int): the number of actions in the game environment to predict.\n",
    "    '''\n",
    "    def __init__(self, n_actions) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        # Convolution layers \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        linear_input_size = 6 * 7 * 32\n",
    "        \n",
    "        # fc layers\n",
    "        self.MLP1 = nn.Linear(linear_input_size, 50)\n",
    "        self.MLP3 = nn.Linear(50, 50)\n",
    "        self.MLP4 = nn.Linear(50, n_actions)\n",
    "        \n",
    "        #Dropout (set to zero)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.dropout_cnn = nn.Dropout(0.0)\n",
    "        \n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        '''\n",
    "        Feeds the input through the model\n",
    "        \n",
    "        x (ndarray 6x7): board state to feed through the model\n",
    "        \n",
    "        returns (torch.tensor): The action for the state\n",
    "        '''\n",
    "        x = self.dropout_cnn(F.leaky_relu(self.conv1(x)))\n",
    "        x = self.dropout_cnn(F.leaky_relu(self.conv2(x)))\n",
    "        x = self.dropout_cnn(F.leaky_relu(self.conv3(x)))\n",
    "\n",
    "        # flatten the feature vector except batch dimension\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.leaky_relu(self.MLP1(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.MLP3(x)))\n",
    "        return self.MLP4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb3f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    '''\n",
    "    This is the class for the DQN agent.\n",
    "    \n",
    "    n_actions (int): number of actions which determine the output dimensionality of the dqn.\n",
    "    lr (float): the learning rate for the optimizer\n",
    "    replay_size (int): the size of the replay buffer\n",
    "    '''\n",
    "    def __init__(self, n_actions, lr, replay_size) -> None:\n",
    "        #Define policy-net, target-net, optimizer and memory for playing as player1 and player2\n",
    "        self.policy_net = DQN(n_actions).to(device)\n",
    "        # target_net will be updated with the polyak average in the training\n",
    "        self.target_net = DQN(n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.policy_net2 = DQN(n_actions).to(device)\n",
    "        self.target_net2 = DQN(n_actions).to(device)\n",
    "        self.target_net2.load_state_dict(self.policy_net2.state_dict())\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr = lr, amsgrad=True)\n",
    "        self.optimizer2 = optim.AdamW(self.policy_net2.parameters(), lr = lr, amsgrad=True)\n",
    "\n",
    "        self.memory = ReplayMemory(replay_size)\n",
    "        self.memory2 = ReplayMemory(replay_size)\n",
    "\n",
    "    def select_action(self, state, available_actions, EPS = 0, steps_done=None, training=True) -> int:\n",
    "        '''\n",
    "        This function selects the action for the given states in an epsilon greedy way.\n",
    "        \n",
    "        state (ndarray 6x7): the current state that is observerd.\n",
    "        available_actions (ndarray 6): the allowed actions in the environment.\n",
    "        EPS (float): the epsilon parameter for exploration\n",
    "        steps_done (int): the number of steps that are already done.\n",
    "        training (boolean): controls the epsilon greedy policy.\n",
    "        \n",
    "        returns (int): the action with the highest q-value for the given state.\n",
    "        '''\n",
    "        # set the state\n",
    "        if state.sum() == 1:\n",
    "            state = -state\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        \n",
    "        # Decide for greedy or random decision\n",
    "        if training == True:\n",
    "            act = np.random.choice(['model','random'], 1, p=[1-EPS, EPS])[0]\n",
    "        else:\n",
    "            act = 'model'\n",
    "        \n",
    "        # follow epsilon-greedy policy\n",
    "        if act == 'model':\n",
    "            with torch.no_grad():\n",
    "                # action recommendations from policy net\n",
    "                if state.sum() == 0:\n",
    "                    r_actions = self.policy_net(state)[0, :]\n",
    "                else:\n",
    "                    r_actions = self.policy_net2(state)[0, :]\n",
    "                state_action_values = [r_actions[action] for action in available_actions]\n",
    "                for i in range(len(state_action_values)):\n",
    "                    state_action_values[i] = state_action_values[i].cpu()\n",
    "                argmax_action = np.argmax(state_action_values)\n",
    "                greedy_action = available_actions[argmax_action]\n",
    "                return greedy_action\n",
    "        # choose random action\n",
    "        else:\n",
    "            return random.choice(available_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
