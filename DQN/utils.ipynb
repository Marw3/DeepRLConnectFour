{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2ae90b",
   "metadata": {},
   "source": [
    "## utils\n",
    "\n",
    "---\n",
    "\n",
    "> Internship neural networks\n",
    ">\n",
    "> Group 4: Reinforcement learning\n",
    ">\n",
    "> Deadline 28.02.23 23:59\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "749fb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d9e1a",
   "metadata": {},
   "source": [
    "# Create replay buffer memory\n",
    "\n",
    "This is the priorized experience replay buffer (PER)\n",
    "One transition is a tuple (s,a,r,s')\n",
    "\n",
    "- Samples transitions according to their priority\n",
    "- Automatically remove old values\n",
    "- Sample batch_size many uncorrelated and not repeated transitions from the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed0d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    '''\n",
    "    The prioritized replay buffer of an agent. The replay buffer enables uncorrelated training of the \n",
    "    dqn and helps therefore to stabilize the training and avoid catastrophic forgetting.\n",
    "    \n",
    "    capacity (float): the capacity of transitions that fit into the replay buffer\n",
    "    alpha (float): how strongly the priorization affects the sampling\n",
    "    '''\n",
    "\n",
    "    def __init__(self, capacity, alpha=0.8) -> None:\n",
    "        self.memory = []\n",
    "        self.priorities = np.array([])\n",
    "        self.prioritized_sampling_prob = np.ones(capacity)/capacity\n",
    "        self.alpha = alpha\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, *args) -> None:\n",
    "        \"\"\"\n",
    "        Pushes a transition into the replay buffer\n",
    "        \n",
    "        args: The tuple (s,a,r,s',d, p) for one transition\n",
    "        \"\"\"\n",
    "        priority = 1.0 if len(self.memory) == 0 else np.median(self.priorities)\n",
    "        if(len(self.priorities) == self.capacity):\n",
    "            idx = self.priorities.argmin()\n",
    "            self.priorities[idx] = priority\n",
    "            self.memory[idx] = Transition(*args)\n",
    "        else:\n",
    "            self.priorities = np.append(self.priorities, priority)\n",
    "            self.memory.append(Transition(*args))\n",
    "            \n",
    "        assert(len(self.memory) == len(self.priorities))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Samples a random batch of transitions from the replay buffer\n",
    "        \n",
    "        batch_size: The size of the batch we sample\n",
    "        \n",
    "        returns: sampled_batch, list of indices\n",
    "        '''\n",
    "        #priorities = np.array(self.memory[\"priority\"])\n",
    "        priorities = self.priorities**self.alpha\n",
    "        sampling_prob = priorities * (1/ priorities.sum())\n",
    "        self.prioritized_sampling_prob = sampling_prob\n",
    "        \n",
    "        indices = np.random.choice(np.array(range(len(self.memory)), dtype=np.int64), batch_size, p=sampling_prob)\n",
    "        \n",
    "        return [self.memory[i] for i in indices], indices\n",
    "    \n",
    "    def update_priorities(self, indices, priorities) -> None:\n",
    "        '''\n",
    "        Updates the priorities by the difference between expected and actual q values(called in the optimizer)\n",
    "        \n",
    "        indices (int[]): indices of the batch\n",
    "        priorities (float[]): new priorities for the transitions\n",
    "        '''\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "            \n",
    "    def pop(self) -> None:\n",
    "        '''\n",
    "        Pops the last sample from the replay buffer\n",
    "        '''\n",
    "        #batch = Transition(*zip(*transitions))\n",
    "        self.priorities = self.priorities[:-1]\n",
    "        self.memory = self.memory[:-1]\n",
    "        self.alpha*=0.99995\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the length of the transitions that are currently in the replay buffer\n",
    "        '''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d7935",
   "metadata": {},
   "source": [
    "# Creating plot for win_rate and average steps taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1586723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(th):\n",
    "    '''\n",
    "    Generates plots for the training history\n",
    "    \n",
    "    - Plot for the winrate with respect to the episode number\n",
    "    - Plot for the average steps to win wrt. the episode number\n",
    "    '''\n",
    "    plt.plot(th[:, 0], th[:, 1], c='c')\n",
    "    win_rate_moving_average = np.convolve(th[:,1], np.ones(8)/8, mode='valid')\n",
    "    plt.plot(np.linspace(100, 10000, len(win_rate_moving_average)), win_rate_moving_average, c='b', label='moving average of win rate')\n",
    "    plt.legend()\n",
    "    plt.title('Playing against random agent')\n",
    "    plt.xlabel('Episode no.')\n",
    "    plt.ylabel('Win rate')\n",
    "    plt.show()\n",
    "    plt.savefig(\"winrate.png\")\n",
    "\n",
    "\n",
    "    plt.plot(th[:, 0], th[:, 2], c='c')\n",
    "    win_steps_taken_moving_average = np.convolve(th[:,2], np.ones(8)/8, mode='valid')\n",
    "    plt.plot(np.linspace(100, 10000, len(win_rate_moving_average)), win_steps_taken_moving_average, c='b', label='moving average of win steps taken')\n",
    "    plt.legend()\n",
    "    plt.title('Playing against random agent')\n",
    "    plt.xlabel('Episode no.')\n",
    "    plt.ylabel('Average steps taken for a win')\n",
    "    plt.show()\n",
    "    plt.savefig(\"avg_steps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20da54",
   "metadata": {},
   "source": [
    "### Testing the win rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f520af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win rate test\n",
    "def win_rate_test(player1, player2, number_of_games):\n",
    "    '''\n",
    "    Tests the win rate of the agents\n",
    "    \n",
    "    player1: The first player\n",
    "    player2: The second player\n",
    "    number_of_games: number of times the win rate is tested\n",
    "    \n",
    "    returns: relative win rate and average win moves for both players\n",
    "    '''\n",
    "    win_moves_taken_list_p1 = []\n",
    "    win_moves_taken_list_p2 = []\n",
    "    wins_p1 = 0\n",
    "    wins_p2 = 0\n",
    "    \n",
    "    # play games and collect data who won\n",
    "    for i in range(number_of_games):\n",
    "        env.reset()\n",
    "        win_moves_taken = 0\n",
    "\n",
    "        while not env.isDone:\n",
    "            state = env.board_state.copy()\n",
    "            available_actions = env.get_available_actions()\n",
    "            action = player1.select_action(state, available_actions, training=False)\n",
    "            state, reward = env.make_move(action, 'p1')\n",
    "            win_moves_taken += 1\n",
    "                \n",
    "            if reward == 10:\n",
    "                win_moves_taken_list_p1.append(win_moves_taken)\n",
    "                wins_p1 += 1\n",
    "                break\n",
    "\n",
    "            available_actions = env.get_available_actions()\n",
    "            action = player2.select_action(state, available_actions, training=False)\n",
    "            state, reward = env.make_move(action, 'p2')\n",
    "            \n",
    "            if reward == 10:\n",
    "                win_moves_taken_list_p2.append(win_moves_taken)\n",
    "                wins_p2 += 1\n",
    "                break\n",
    "    \n",
    "    average_win_moves_taken_p1 = -1\n",
    "    average_win_moves_taken_p2 = -1\n",
    "    \n",
    "    if len(win_moves_taken_list_p1) > 0:\n",
    "        average_win_moves_taken_p1 = sum(win_moves_taken_list_p1)/len(win_moves_taken_list_p1)\n",
    "    if len(win_moves_taken_list_p2) > 0:\n",
    "        average_win_moves_taken_p2 = sum(win_moves_taken_list_p2)/len(win_moves_taken_list_p2)\n",
    "        \n",
    "    statistic = (wins_p1, wins_p2, number_of_games - (wins_p1 + wins_p2))\n",
    "    \n",
    "    print(\"Absolute wins_p1, wins_p2, draws: \", statistic)\n",
    "    print(\"Relative wins_p1, wins_p2, draws \", list(map(lambda s: s/number_of_games,statistic)))\n",
    "\n",
    "\n",
    "    return wins_p1/number_of_games, average_win_moves_taken_p1, wins_p2/number_of_games, average_win_moves_taken_p2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
