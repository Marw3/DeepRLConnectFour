{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c633be7",
   "metadata": {},
   "source": [
    "## Training and evaluation of DQN\n",
    "\n",
    "---\n",
    "\n",
    "> Internship neural networks\n",
    ">\n",
    "> Group 4: Reinforcement learning\n",
    ">\n",
    "> Deadline 28.02.23 23:59\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d13d893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5984d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../Environment/Connect4.ipynb\"\n",
    "%run \"../utils/utils.ipynb\"\n",
    "%run \"../OtherAgents/Agents.ipynb\"\n",
    "%run \"../utils/utils.ipynb\"\n",
    "%run \"DQN.ipynb\"\n",
    "%run \"utils.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e42967",
   "metadata": {},
   "source": [
    "# Hyperparameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cf8dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 1 # epsilon decay parameters\n",
    "EPS_MIN = 0.05\n",
    "EPS_DELTA = 0.9998\n",
    "\n",
    "BATCH_SIZE = 64 # numbers of samples to draw from the replay buffer\n",
    "GAMMA = 0.9 # discount of the rewards\n",
    "TAU = 0.005 # polyak average update rate for the target network\n",
    "lr = 1e-4 # learning rate for \n",
    "\n",
    "REPLAY_SIZE = 10000 # number of transitions that can be stored in the replay buffer shouldn't be too small\n",
    "\n",
    "NUM_EPISODES = 40000 # the number of games in the training\n",
    "\n",
    "torch.manual_seed(42) # setting seed to reproduce results\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6408571",
   "metadata": {},
   "source": [
    "# Set the device (cuda or cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42e9c228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46f81d",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Define environment, dqnagent and opponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a42a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Connect4()\n",
    "\n",
    "# get max no. of actions from action space\n",
    "n_actions = env.board_width\n",
    "\n",
    "dqn = DQNAgent(n_actions, lr, REPLAY_SIZE)\n",
    "\n",
    "randomPlayer = RandomAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb257c",
   "metadata": {},
   "source": [
    "# Update function\n",
    "\n",
    "Updates the DQN policy and target net according to the difference in the q-values and the expected q-values\n",
    "\n",
    "- Double DQN\n",
    "- Updates the priorities for the batch\n",
    "- Backpropagates the loss through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3634304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(optimizer, memory, policy_net, target_net) -> torch.tensor:\n",
    "    '''\n",
    "    The optimization function of the DQN.\n",
    "    \n",
    "    optimizer: the optimizer of the agent to update the parameters\n",
    "    memory: the replay memory of the agent with the transitions\n",
    "    policy_net: the policy net of the agent to calculate the q-values\n",
    "    target_net: the target net of the agent to get the target q-values (expected q values)\n",
    "    \n",
    "    returns: the loss for the batch\n",
    "    '''\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Get the transitions from the batch\n",
    "    transitions, indices = memory.sample(BATCH_SIZE)\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = zip(*[(np.expand_dims(m[0], axis=0), \\\n",
    "                                    [m[1]], m[2], np.expand_dims(m[3], axis=0)) for m in transitions])\n",
    "    # tensor wrapper\n",
    "    state_batch = torch.tensor(np.array(state_batch), dtype=torch.float, device=device)\n",
    "    reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float, device=device)\n",
    "    action_batch = torch.tensor(np.asarray(action_batch), dtype=torch.int64, device=device)\n",
    "    \n",
    "    # for assigning terminal state value = 0 later\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s_: s_[0] is not None, next_state_batch)), device=device)\n",
    "    non_final_next_states = [torch.tensor(s_, dtype=torch.float, device=device).unsqueeze(0) for s_ in next_state_batch if s_[0] is not None]\n",
    "    if len(non_final_next_states) > 0:\n",
    "        non_final_next_states = torch.cat(non_final_next_states)\n",
    "\n",
    "    # prediction from policy_net (q-values for the actions)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            next_state_actions = torch.argmax(policy_net(non_final_next_states).detach(), dim=1)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1).detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Update the priorities according to the difference in the q-values\n",
    "    delta = abs(expected_state_action_values.unsqueeze(1) - state_action_values) + 1e-5\n",
    "    memory.update_priorities(indices, delta)\n",
    "    \n",
    "    loss = F.huber_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7a79c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid resetting\n",
    "steps_done = 0\n",
    "training_history = []\n",
    "training_history_player2 = []\n",
    "WIN_RATE_TEST = 1000\n",
    "LOSSES = []\n",
    "NUM_EPISODES = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65373a27",
   "metadata": {},
   "source": [
    "# Training function\n",
    "\n",
    "The function helps training the DQN network against the opponents.\n",
    "\n",
    "- MixedTraining is the training against NegaMax opoonents of various depth interchangeably\n",
    "- Play games against another player with randomly assigning the player position\n",
    "- Updates the policy net after each game\n",
    "- Polyak average for target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d15934be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(p1, p2 = None, EPS = 1, num_episodes = 1000):\n",
    "    '''\n",
    "    The training function. \n",
    "    \n",
    "    - Agent plays games against an opponent\n",
    "    - Updates the parameters of the network\n",
    "    - Saves data and model parameters\n",
    "    \n",
    "    p1: A player\n",
    "    p2: Another player\n",
    "    EPS: The epsilon for the exploration\n",
    "    num_episodes: How many games will be played\n",
    "    \n",
    "    returns: the training history\n",
    "    '''\n",
    "    \n",
    "    # Define parameters for the training\n",
    "    steps_done = 0\n",
    "    depth = 0\n",
    "    mixedTraining = False\n",
    "    if p2 == None:\n",
    "        mixedTraining = True\n",
    "    \n",
    "    # Loop for the games to play\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # record every 1000 epochs the win rate of the agent as player1 and player2 respectively\n",
    "        if i % WIN_RATE_TEST == WIN_RATE_TEST-1:\n",
    "            print(\"Player 1:\")\n",
    "            win_rate, movestaken, _, _ = win_rate_test(p1, p2, 100)\n",
    "            training_history.append([i + 1, win_rate, movestaken])\n",
    "            print(\"Player 2:\")\n",
    "            _, _, win_rate_p2, moves_taken_p2 = win_rate_test(p2, p1, 100)\n",
    "            training_history_player2.append([i + 1, win_rate_p2, moves_taken_p2])\n",
    "            \n",
    "        # Reset the environment\n",
    "        env.reset()\n",
    "        state_p1 = env.board_state.copy()\n",
    "        \n",
    "        # For the mixed training choose the depth of the negamax\n",
    "        if mixedTraining:\n",
    "            if i % WIN_RATE_TEST == WIN_RATE_TEST-1:\n",
    "                print(\"depth: \", depth)\n",
    "            depth = np.random.choice([2,3,4])\n",
    "            p2 = NegaMaxAgent(env, depth)\n",
    "            \n",
    "        # Define player order randomly for the dqn to play as both players\n",
    "        j = np.random.choice([0,1])\n",
    "        if j == 0:\n",
    "            player1 = p1\n",
    "            player2 = p2\n",
    "        else:\n",
    "            player1 = p2\n",
    "            player2 = p1\n",
    "        \n",
    "        # Loop over one game    \n",
    "        for t in count():\n",
    "            # First player select action and make a move in the environment\n",
    "            available_actions = env.get_available_actions()\n",
    "            action_p1 = player1.select_action(state_p1, available_actions, EPS, steps_done)\n",
    "            steps_done += 1\n",
    "            state_p1_, reward_p1 = env.make_move(action_p1, 'p1', isDqn = True)\n",
    "            \n",
    "            # Check if environment is done and push into memory\n",
    "            if env.isDone:\n",
    "                if reward_p1 == 0:\n",
    "                    # state action value tuple for a draw\n",
    "                    if player1.memory != None:\n",
    "                        player1.memory.push(state_p1.copy(), action_p1, reward_p1, None)\n",
    "                    if player2.memory != None:\n",
    "                        player2.memory2.push(-temp.copy(), action_p2, reward_p1, None)\n",
    "                else:\n",
    "                    # reward p1 for p1's win\n",
    "                    if player1.memory != None:\n",
    "                        player1.memory.push(state_p1.copy(), action_p1, reward_p1, None)\n",
    "                    if player2.memory != None:\n",
    "                        player2.memory2.push(-temp.copy(), action_p2, -reward_p1, None)\n",
    "                break\n",
    "            \n",
    "            # Second player select action and make a move in the environment\n",
    "            available_actions = env.get_available_actions()\n",
    "            action_p2 = player2.select_action(state_p1_, available_actions, EPS, steps_done)\n",
    "            state_p2_, reward_p2 = env.make_move(action_p2, \"p2\", isDqn = True)\n",
    "\n",
    "            # Check if environment is done and push into memory\n",
    "            if env.isDone:\n",
    "                if reward_p2 == 0:\n",
    "                    # state action value tuple for a draw\n",
    "                    if player1.memory != None:\n",
    "                        player1.memory.push(state_p1.copy(), action_p1, reward_p2, None)\n",
    "                    if player2.memory != None:\n",
    "                        player2.memory2.push(-state_p1_.copy(), action_p2, reward_p2, None)\n",
    "                else:\n",
    "                    # punish p1 for (random agent) p2's win \n",
    "                    if player1.memory != None:\n",
    "                        player1.memory.push(state_p1.copy(), action_p1, -reward_p2, None)\n",
    "                    if player2.memory != None:\n",
    "                        player2.memory2.push(-state_p1_.copy(), action_p2, reward_p2, None)\n",
    "                break\n",
    "                \n",
    "            # Push experience into memory (negative state for player2 network)\n",
    "            if t != 0:\n",
    "                if player2.memory != None:\n",
    "                    player2.memory2.push(-temp.copy(), temp1, reward_p2, -state_p1_.copy())\n",
    "            if player1.memory != None:\n",
    "                player1.memory.push(state_p1.copy(), action_p1, reward_p1, state_p2_.copy())\n",
    "                \n",
    "            # Copy temporal information for next step memory\n",
    "            state_p1 = state_p2_\n",
    "            temp = state_p1_.copy()\n",
    "            temp1 = action_p2\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        if player1.policy_net != None:\n",
    "            loss = optimize_model(player1.optimizer, player1.memory, player1.policy_net, player1.target_net)\n",
    "            LOSSES.append(loss)\n",
    "\n",
    "        if player2.policy_net != None:\n",
    "            loss = optimize_model(player2.optimizer2, player2.memory2, player2.policy_net2, player2.target_net2)\n",
    "            LOSSES.append(loss)\n",
    "                \n",
    "        # Update the epsilon\n",
    "        EPS = EPS * EPS_DELTA\n",
    "        EPS = max(EPS_MIN, EPS)\n",
    "        \n",
    "        # Reset player positions\n",
    "        if j == 0:\n",
    "            p1 = player1\n",
    "            p2 = player2\n",
    "        else:\n",
    "            p2 = player1\n",
    "            p1 = player2\n",
    "            \n",
    "        # Soft update of the target network's weights (for both player networks)\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = p1.target_net.state_dict()\n",
    "        policy_net_state_dict = p1.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        p1.target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        target_net_state_dict = p1.target_net2.state_dict()\n",
    "        policy_net_state_dict = p1.policy_net2.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        p1.target_net2.load_state_dict(target_net_state_dict)\n",
    "          \n",
    "        # Save model after 10000 iterations\n",
    "        if i % 10000 == 0:\n",
    "            path = FOLDER + '/DQN_Epochs' + str(i) + '_player1.pth'\n",
    "            torch.save(p1.policy_net.state_dict(), path)\n",
    "            path = FOLDER + '/DQN_Epochs' + str(i) + '_player2.pth'\n",
    "            torch.save(p1.policy_net2.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a220ceb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df461bf8",
   "metadata": {},
   "source": [
    "## Training against the random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5eb5362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                      | 221/40000 [00:06<19:56, 33.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m dqn\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpriorities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m      8\u001b[0m dqn\u001b[38;5;241m.\u001b[39mmemory2\u001b[38;5;241m.\u001b[39mpriorities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomPlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPS_START\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPISODES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m th \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(training_history)\n\u001b[1;32m     11\u001b[0m th_p2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(training_history_player2)\n",
      "Cell \u001b[0;32mIn [31], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(p1, p2, EPS, num_episodes)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Second player select action and make a move in the environment\u001b[39;00m\n\u001b[1;32m     79\u001b[0m available_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_available_actions()\n\u001b[0;32m---> 80\u001b[0m action_p2 \u001b[38;5;241m=\u001b[39m \u001b[43mplayer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_p1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_done\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m state_p2_, reward_p2 \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mmake_move(action_p2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2\u001b[39m\u001b[38;5;124m\"\u001b[39m, isDqn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Check if environment is done and push into memory\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/ipykernel_40632/3059359913.py:41\u001b[0m, in \u001b[0;36mDQNAgent.select_action\u001b[0;34m(self, state, available_actions, EPS, steps_done, training)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mstate\n\u001b[0;32m---> 41\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Decide for greedy or random decision\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FOLDER = \"final_dqn_against_random\"\n",
    "\n",
    "if not os.path.isdir(FOLDER):\n",
    "    os.mkdir(FOLDER)\n",
    "dqn.memory.memory.clear()\n",
    "dqn.memory2.memory.clear()\n",
    "dqn.memory.priorities = np.array([])\n",
    "dqn.memory2.priorities = np.array([])\n",
    "train(dqn, randomPlayer, EPS_START, NUM_EPISODES)\n",
    "th = np.array(training_history)\n",
    "th_p2 = np.array(training_history_player2)\n",
    "write_list(th_p2, FOLDER + \"/training_history_player2_path\")\n",
    "write_list(th, FOLDER + \"/training_history_path\")\n",
    "write_list(LOSSES, FOLDER + \"/losses\")\n",
    "path = FOLDER + '/FINAL_DQN_Epochs' + str(NUM_EPISODES) + '_player1.pth'\n",
    "torch.save(dqn.policy_net.state_dict(), path)\n",
    "path = FOLDER + '/FINAL_DQN_Epochs' + str(NUM_EPISODES) + '_player2.pth'\n",
    "torch.save(dqn.policy_net2.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039de0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fe080",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(th_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a1c6f",
   "metadata": {},
   "source": [
    "## Training against the NegaMaxAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff5035e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                      | 997/40000 [00:35<26:07, 24.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1:\n",
      "Absolute wins_p1, wins_p2, draws:  (20, 80, 0)\n",
      "Relative wins_p1, wins_p2, draws  [0.2, 0.8, 0.0]\n",
      "Player 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▉                                   | 1004/40000 [00:40<4:08:36,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute wins_p1, wins_p2, draws:  (88, 10, 2)\n",
      "Relative wins_p1, wins_p2, draws  [0.88, 0.1, 0.02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                    | 1999/40000 [01:18<25:08, 25.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1:\n",
      "Absolute wins_p1, wins_p2, draws:  (23, 77, 0)\n",
      "Relative wins_p1, wins_p2, draws  [0.23, 0.77, 0.0]\n",
      "Player 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|█▊                                  | 2002/40000 [01:24<6:30:12,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute wins_p1, wins_p2, draws:  (96, 2, 2)\n",
      "Relative wins_p1, wins_p2, draws  [0.96, 0.02, 0.02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                   | 2996/40000 [02:04<24:59, 24.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1:\n",
      "Absolute wins_p1, wins_p2, draws:  (21, 79, 0)\n",
      "Relative wins_p1, wins_p2, draws  [0.21, 0.79, 0.0]\n",
      "Player 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██▋                                 | 3003/40000 [02:11<4:54:28,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute wins_p1, wins_p2, draws:  (98, 1, 1)\n",
      "Relative wins_p1, wins_p2, draws  [0.98, 0.01, 0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                  | 3999/40000 [02:54<27:20, 21.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1:\n",
      "Absolute wins_p1, wins_p2, draws:  (5, 95, 0)\n",
      "Relative wins_p1, wins_p2, draws  [0.05, 0.95, 0.0]\n",
      "Player 2:\n",
      "Absolute wins_p1, wins_p2, draws:  (90, 6, 4)\n",
      "Relative wins_p1, wins_p2, draws  [0.9, 0.06, 0.04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▉                                  | 4128/40000 [03:08<27:18, 21.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m dqn\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpriorities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m      9\u001b[0m dqn\u001b[38;5;241m.\u001b[39mmemory2\u001b[38;5;241m.\u001b[39mpriorities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnega_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPISODES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m th \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(training_history)\n\u001b[1;32m     12\u001b[0m th_p2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(training_history_player2)\n",
      "Cell \u001b[0;32mIn [20], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(p1, p2, EPS, num_episodes)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     61\u001b[0m available_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_available_actions()\n\u001b[0;32m---> 62\u001b[0m action_p2 \u001b[38;5;241m=\u001b[39m \u001b[43mplayer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_p1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_done\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m state_p2_, reward_p2 \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mmake_move(action_p2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2\u001b[39m\u001b[38;5;124m\"\u001b[39m, isDqn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39misDone:\n",
      "File \u001b[0;32m/tmp/ipykernel_40632/4192771985.py:34\u001b[0m, in \u001b[0;36mDQNAgent.select_action\u001b[0;34m(self, state, available_actions, EPS, steps_done, training)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m act \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# action recommendations from policy net\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m             r_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net(state)[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FOLDER = \"final_dqn_against_negaMax2\"\n",
    "\n",
    "if not os.path.isdir(FOLDER):\n",
    "    os.mkdir(FOLDER)\n",
    "nega_max = NegaMaxAgent(env, 2)\n",
    "dqn.memory.memory.clear()\n",
    "dqn.memory2.memory.clear()\n",
    "dqn.memory.priorities = np.array([])\n",
    "dqn.memory2.priorities = np.array([])\n",
    "train(dqn, nega_max, 0.7, NUM_EPISODES)\n",
    "th = np.array(training_history)\n",
    "th_p2 = np.array(training_history_player2)\n",
    "write_list(th_p2, FOLDER + \"/training_history_player2_path\")\n",
    "write_list(th, FOLDER + \"/training_history_path\")\n",
    "write_list(LOSSES, FOLDER + \"/losses\")\n",
    "path = FOLDER + '/FINAL_DQN_Epochs' + str(NUM_EPISODES) + '_player1.pth'\n",
    "torch.save(dqn.policy_net.state_dict(), path)\n",
    "path = FOLDER + '/FINAL_DQN_Epochs' + str(NUM_EPISODES) + '_player2.pth'\n",
    "torch.save(dqn.policy_net2.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(th_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd44c7",
   "metadata": {},
   "source": [
    "## Mixtraining\n",
    "\n",
    "- Training against negamax agents of different depths (random choise each game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517e073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FOLDER = \"final_dqn_against_negaMaxMix\"\n",
    "\n",
    "if not os.path.isdir(FOLDER):\n",
    "    os.mkdir(FOLDER)\n",
    "\n",
    "train(p1 = dqn, EPS = 0.5, num_episodes = 20000)\n",
    "th = np.array(training_history)\n",
    "th_p2 = np.array(training_history_player2)\n",
    "write_list(th_p2, FOLDER + \"/training_history_player2_path\")\n",
    "write_list(th, FOLDER + \"/training_history_path\")\n",
    "write_list(LOSSES, FOLDER + \"/losses\")\n",
    "path = FOLDER + '/FINAL_DQN_Epochs' + str(NUM_EPISODES) + '_player1.pth'\n",
    "torch.save(dqn.policy_net.state_dict(), path)\n",
    "path = FOLDER + '/FINAL_DQN_Epochs' + str(NUM_EPISODES) + '_player2.pth'\n",
    "torch.save(dqn.policy_net2.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fe946",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abeb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(th_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18376445",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
