{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02366851",
   "metadata": {},
   "source": [
    "## Decision Transformer\n",
    "\n",
    "---\n",
    "\n",
    "> Internship neural networks\n",
    ">\n",
    "> Group 4: Reinforcement learning\n",
    ">\n",
    "> Deadline 28.02.23 23:59\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/kzl/decision-transformer\n",
    "# source: https://github.com/nikhilbarhate99/min-decision-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdcae8",
   "metadata": {},
   "source": [
    "# Define decision transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    '''\n",
    "    This is the masked causal attention for our decision transformer\n",
    "\n",
    "    hidden_dim: hidden dimension of transformer\n",
    "    input_seq_len: 3 (states, actions, rtgs) * length of trajectory\n",
    "    n_heads: number of attention heads\n",
    "    drop_p: dropout probability for attention and projection\n",
    "    '''\n",
    "    def __init__(self, hidden_dim, input_seq_len, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads # number of heads\n",
    "\n",
    "        # layers for value, key and query of attention\n",
    "        self.value_net = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_net = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.query_net = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.proj_net = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # create Mask\n",
    "        mask = torch.tril(torch.ones((input_seq_len, input_seq_len))).view(1, 1, input_seq_len, input_seq_len)\n",
    "\n",
    "        # register buffer prevent mask to get updated while backpropagation \n",
    "        self.register_buffer('mask',mask)\n",
    "        \n",
    "        # regularization\n",
    "        self.att_drop = nn.Dropout(drop_p)\n",
    "        self.proj_drop = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, traj_length, C = x.shape # batch size, seq length, hidden_dim * n_heads\n",
    "\n",
    "        att_dim = C // self.n_heads # attention dim\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and rearrange to (batch_size, n_heads, traj_length, att_dim) \n",
    "        values = self.value_net(x).view(batch_size, traj_length, self.n_heads, att_dim).transpose(1,2)\n",
    "        keys = self.key_net(x).view(batch_size, traj_length, self.n_heads, att_dim).transpose(1,2)\n",
    "        queries = self.query_net(x).view(batch_size, traj_length, self.n_heads, att_dim).transpose(1,2)\n",
    "\n",
    "        attention_weights = queries @ keys.transpose(2,3) / math.sqrt(att_dim)\n",
    "        # apply causal mask to weights\n",
    "        attention_weights = attention_weights.masked_fill(self.mask[...,:traj_length,:traj_length] == 0, float('-inf'))\n",
    "        \n",
    "        # normalize with softmax\n",
    "        normalized_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        # attention (batch_size, n_heads, traj_length, D)\n",
    "        attention = self.att_drop(normalized_weights @ values)\n",
    "\n",
    "        # gather heads and project (batch_size, n_heads, traj_length, att_dim) -> (batch_size, traj_length, C)\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, traj_length,C)\n",
    "\n",
    "        return self.proj_drop(self.proj_net(attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0949d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''\n",
    "    This is the transformer block\n",
    "\n",
    "    hidden_dim: hidden dimension of transformer\n",
    "    input_seq_len: 3 (states, actions, rtgs) * length of trajectory\n",
    "    n_heads: number of attention heads\n",
    "    drop_p: dropout probability for attention and projection\n",
    "    '''\n",
    "    def __init__(self, hidden_dim, input_seq_len, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.attention = CausalAttention(hidden_dim, input_seq_len, n_heads, drop_p)\n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 4*hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4*hidden_dim, hidden_dim),\n",
    "                nn.Dropout(drop_p),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(x)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mlp(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformer(nn.Module):\n",
    "    '''\n",
    "    This is the decision transformer\n",
    "\n",
    "    state_dim: dimension of the states\n",
    "    act_dim: dimension of the actions\n",
    "    n_blocks: number of transformer blocks\n",
    "    hidden_dim: hidden dimension of transformer\n",
    "    context_len: length of the context our decision transformer looks at\n",
    "    n_heads: number of attention heads\n",
    "    drop_p: dropout probability for attention and projection\n",
    "    vocab_size: number of possible actions\n",
    "    max_timestep: maximum length of a game\n",
    "    '''\n",
    "    def __init__(self, state_dim, act_dim, n_blocks, hidden_dim, context_len,\n",
    "                 n_heads, drop_p, vocab_size, max_timestep=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_dim = act_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ### transformer\n",
    "        input_seq_len = 3 * context_len\n",
    "        blocks = [Block(hidden_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
    "        self.transformer = nn.Sequential(*blocks)\n",
    "        \n",
    "        ### prediction heads\n",
    "        self.predict_rtg = nn.Linear(hidden_dim, 1)\n",
    "        self.predict_state = nn.Linear(hidden_dim, state_dim)\n",
    "        self.predict_action = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
    "        self.SL = nn.Softmax(dim = 2)\n",
    "        \n",
    "        # embedding for actions\n",
    "        self.action_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        ### embeddings for projection\n",
    "        self.embedding_layer = nn.LayerNorm(hidden_dim)\n",
    "        self.t_embedding = nn.Embedding(max_timestep, hidden_dim)\n",
    "        self.rtg_embedding = nn.Linear(1, hidden_dim)\n",
    "        self.state_embedding = nn.Linear(state_dim, hidden_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, timesteps, states, actions, returns_to_go, traj_mask=None):\n",
    "\n",
    "        batch_size, traj_length, _ = states.shape\n",
    "        \n",
    "        # embeddings\n",
    "        time_embeddings = self.t_embedding(timesteps)\n",
    "        state_embeddings = self.state_embedding(states) + time_embeddings\n",
    "        action_embeddings = self.action_embedding(actions) + time_embeddings\n",
    "        returns_embeddings = self.rtg_embedding(returns_to_go) + time_embeddings\n",
    "        \n",
    "        # get rtg, states and actions in form (r_0, s_0, a_0, r_1, s_1, a_1, ...)\n",
    "        h = torch.stack((returns_embeddings, state_embeddings, action_embeddings), dim=1).permute(0, 2, 1, 3).reshape(batch_size, 3 * traj_length, self.hidden_dim)\n",
    "\n",
    "        # transformer and prediction\n",
    "        h = self.transformer(h)\n",
    "        \n",
    "        h = self.embedding_layer(h)\n",
    "\n",
    "        # predict action given r, s\n",
    "        action_preds = self.predict_action(h.reshape(batch_size, traj_length, 3, self.hidden_dim).permute(0, 2, 1, 3)[:,1])\n",
    "        action_target = torch.clone(actions).detach().to(device).to(torch.int64)\n",
    "\n",
    "        loss = None\n",
    "        if traj_mask is not None:\n",
    "            # only consider non padded elements\n",
    "            action_preds_reshape = action_preds.to(torch.float32).view(-1, self.act_dim, vocab_size)[traj_mask.view(-1,) > 0].view(-1, vocab_size)\n",
    "            action_target_reshape = action_target.view(-1, self.act_dim)[traj_mask.view(-1,) > 0].view(-1)\n",
    "            \n",
    "            loss = F.cross_entropy(action_preds_reshape, action_target_reshape, reduction='mean')\n",
    "        \n",
    "        action_preds = self.SL(action_preds)\n",
    "        \n",
    "        return action_preds, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259fb69",
   "metadata": {},
   "source": [
    "# Decision transformer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44920f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTAgent():\n",
    "    '''\n",
    "    This is the agent class for our decision transformer\n",
    "\n",
    "    state_dim: dimension of the states\n",
    "    act_dim: dimension of the actions\n",
    "    n_blocks: number of transformer blocks\n",
    "    hidden_dim: hidden dimension of transformer\n",
    "    context_len: length of the context our decision transformer looks at\n",
    "    n_heads: number of attention heads\n",
    "    drop_p: dropout probability for attention and projection\n",
    "    rtg_target: target reward our decision transformer wants to get\n",
    "    vocab_size: number of possible actions\n",
    "    max_timestep: maximum length of a game\n",
    "    '''\n",
    "    def __init__(self, state_dim, act_dim, n_blocks, hidden_dim, context_len,\n",
    "                 n_heads, drop_p, rtg_target, vocab_size, max_timestep=21):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.n_blocks = n_blocks\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.context_len = context_len\n",
    "        self.n_heads= n_heads\n",
    "        self.drop_p = drop_p\n",
    "        self.rtg_target = rtg_target\n",
    "        self.max_timestep = max_timestep\n",
    "        \n",
    "        self.model =  DecisionTransformer(\n",
    "            state_dim=self.state_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            n_blocks=self.n_blocks,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            context_len=self.context_len,\n",
    "            n_heads=self.n_heads,\n",
    "            drop_p=self.drop_p,\n",
    "            vocab_size = vocab_size).to(device)\n",
    "        \n",
    "        self.timesteps = torch.arange(start=0, end=self.max_timestep, step=1)\n",
    "        self.timesteps = self.timesteps.repeat(1, 1).to(device)\n",
    "        \n",
    "        # zeros place holders\n",
    "        self.actions = torch.zeros((1, self.max_timestep),\n",
    "                                dtype=torch.int32, device=device)\n",
    "\n",
    "        self.states = torch.zeros((1, self.max_timestep, state_dim),\n",
    "                                dtype=torch.float32, device=device)\n",
    "            \n",
    "        self.rewards_to_go = torch.zeros((1, self.max_timestep, 1),\n",
    "                                dtype=torch.float32, device=device)\n",
    "        \n",
    "        self.running_rtg = self.rtg_target\n",
    "        \n",
    "    def select_action(self, t, running_reward, state, available_actions, EPS = 0, againstDQN=True):\n",
    "        '''\n",
    "        selects action for actual state in game\n",
    "\n",
    "        t: timestep\n",
    "        running_reward: actual reward we got in game\n",
    "        state: actual state of the game\n",
    "        available_actions: the available actions/columns to throw a coin in\n",
    "        EPS: epsilon for exploration/explotation ratio (not necessary for dt)\n",
    "        againstDQN: tells if dt plays against dqn -> if yes, action selection over probabilities of the actions to get some variance in the games\n",
    "\n",
    "        returns: action to take\n",
    "        '''\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # add state in placeholder and normalize\n",
    "            self.states[0, t] = torch.from_numpy(state.flatten()).to(torch.float32).to(device)\n",
    "                \n",
    "            # calcualate running rtg and add in placeholder\n",
    "            self.running_rtg = self.running_rtg - (running_reward)\n",
    "            self.rewards_to_go[0, t] = self.running_rtg\n",
    "            \n",
    "            if t < self.context_len:\n",
    "                act_preds, _ = self.model.forward(self.timesteps[:,:self.context_len],\n",
    "                                                    self.states[:,:self.context_len],\n",
    "                                                    self.actions[:,:self.context_len],\n",
    "                                                    self.rewards_to_go[:,:self.context_len])\n",
    "                act = act_preds[0, t].detach()\n",
    "            else:\n",
    "                act_preds, _ = self.model.forward(self.timesteps[:,t-self.context_len+1:t+1],\n",
    "                                                    self.states[:,t-self.context_len+1:t+1],\n",
    "                                                    self.actions[:,t-self.context_len+1:t+1],\n",
    "                                                    self.rewards_to_go[:,t-self.context_len+1:t+1])\n",
    "                act = act_preds[0, -1].detach()\n",
    "            \n",
    "            act = [act[i] for i in available_actions]\n",
    "            for i in range(len(act)):\n",
    "                    act[i] = act[i].cpu()\n",
    "            if againstDQN:\n",
    "                action = random.choices(range(len(act)), act)[0]\n",
    "            else:\n",
    "                action = np.argmax(act)\n",
    "            action = available_actions[action]\n",
    "            act = torch.tensor(action)\n",
    "            \n",
    "            # add action in placeholder\n",
    "            self.actions[0, t] = act.to(torch.int32)\n",
    "            \n",
    "            return action\n",
    "        \n",
    "    def reset_agent(self):\n",
    "        '''\n",
    "        resets the dt for next game\n",
    "        '''\n",
    "        # zeros place holders\n",
    "        self.actions = torch.zeros((1, self.max_timestep),\n",
    "                                dtype=torch.int32, device=device)\n",
    "\n",
    "        self.states = torch.zeros((1, self.max_timestep, self.state_dim),\n",
    "                                dtype=torch.float32, device=device)\n",
    "            \n",
    "        self.rewards_to_go = torch.zeros((1, self.max_timestep, 1),\n",
    "                                dtype=torch.float32, device=device)\n",
    "        \n",
    "        self.running_rtg = self.rtg_target\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
