{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5eae6dc",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "---\n",
    "\n",
    "> Internship neural networks\n",
    ">\n",
    "> Group 4: Reinforcement learning\n",
    ">\n",
    "> Deadline 28.02.23 23:59\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fb87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c634eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../../Environment/Connect4.ipynb\"\n",
    "%run \"../../utils/utils.ipynb\"\n",
    "%run \"../../OtherAgents/Agents.ipynb\"\n",
    "%run \"../utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776f86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Connect4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498a5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(trajectory, agent, opponent, num_episodes = 10):\n",
    "    '''\n",
    "    createDataset fills a list with trajectories (our dataset) where each trajectory represents \n",
    "    one game of connect 4 of two players  \n",
    "\n",
    "    :trajectory: trajectory contains a dict with three keys (observations, actions, return_to_goes)\n",
    "    :agent: our point of view (the agent is the player our decision transformer learns from)\n",
    "    :opponent: opponent\n",
    "    '''\n",
    "    for i in tqdm(range(num_episodes)): \n",
    "        env.reset()\n",
    "        state_p1 = env.board_state.copy()\n",
    "        \n",
    "        # randomly select who is the first and who is the second player\n",
    "        j = np.random.choice([0,1])\n",
    "        if j == 0:\n",
    "            player1 = agent\n",
    "            player2 = opponent\n",
    "        else:\n",
    "            player1 = opponent\n",
    "            player2 = agent\n",
    "\n",
    "        for t in count():\n",
    "            \n",
    "            # select action and make move player 1\n",
    "            available_actions = env.get_available_actions()\n",
    "            action_p1 = player1.select_action(state_p1, available_actions)\n",
    "            state_p2, reward = env.make_move(action_p1, 'p1')\n",
    "            \n",
    "            if env.isDone:\n",
    "                # j tells us if our agent is player 1 or player 2\n",
    "                if j == 0:\n",
    "                    # add the state, action and current reward to our traj\n",
    "                    trajectory.push(state_p1, action_p1, reward)\n",
    "                else:\n",
    "                    # ngeative reward for a lose\n",
    "                    trajectory.pop()\n",
    "                    trajectory.push(state_p2, action_p2, -reward)\n",
    "                \n",
    "                # coumpute rtgs\n",
    "                for i in range(len(trajectory.trajDict[\"rtgs\"])):\n",
    "                    trajectory.trajDict[\"rtgs\"][i] = trajectory.compute_rtgs(i)\n",
    "                # add traj to dataset and reset traj for new game\n",
    "                dataset.append(trajectory.trajDict.copy())\n",
    "                trajectory.reset()\n",
    "                break\n",
    "            \n",
    "            if j == 0:\n",
    "                # add the state, action and current reward to our traj\n",
    "                trajectory.push(state_p1, action_p1, reward)\n",
    "\n",
    "            # select action and make move player 2\n",
    "            available_actions = env.get_available_actions()\n",
    "            action_p2 = player2.select_action(state_p2, available_actions)\n",
    "            next_state_p2, reward = env.make_move(action_p2, 'p2')\n",
    "            \n",
    "            if env.isDone:\n",
    "                if j == 0:\n",
    "                    trajectory.pop()\n",
    "                    trajectory.push(state_p1, action_p1, -reward)\n",
    "                else:\n",
    "                    trajectory.push(state_p2, action_p2, reward)\n",
    "                for i in range(len(trajectory.trajDict[\"rtgs\"])):\n",
    "                    trajectory.trajDict[\"rtgs\"][i] = trajectory.compute_rtgs(i)\n",
    "                dataset.append(trajectory.trajDict.copy())\n",
    "                trajectory.reset()\n",
    "                break\n",
    "            \n",
    "            if j == 1:\n",
    "                trajectory.push(state_p2, action_p2, reward)\n",
    "\n",
    "            state_p1 = next_state_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a3f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 50000/50000 [4:03:55<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24950\n",
      "Done writing list into a binary file\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trajectory = Trajectory()\n",
    "agent = RandomAgent()\n",
    "opponent = RandomAgent()\n",
    "num_episodes = 100000\n",
    "dataset = []\n",
    "createDataset(trajectory, agent, opponent, num_episodes)\n",
    "agentName = agent.__class__.__name__\n",
    "oppName = opponent.__class__.__name__\n",
    "# write dataset into a file\n",
    "write_list(dataset, agentName + \"Vs\" + oppName + \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf247239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 100000/100000 [39:36<00:00, 42.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing list into a binary file\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trajectory = Trajectory()\n",
    "depth = 4\n",
    "agent = NegaMaxAgent(env, depth)\n",
    "opponent = RandomAgent()\n",
    "num_episodes = 100000\n",
    "dataset = []\n",
    "createDataset(trajectory, agent, opponent, num_episodes)\n",
    "agentName = agent.__class__.__name__\n",
    "oppName = opponent.__class__.__name__\n",
    "# write dataset into a file\n",
    "write_list(dataset, agentName + str(depth) + \"Vs\" + oppName + \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12f3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 50000/50000 [11:53:28<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing list into a binary file\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trajectory = Trajectory()\n",
    "depth = 4\n",
    "agent = NegaMaxAgent(env, depth)\n",
    "opponent = NegaMaxAgent(env, depth)\n",
    "num_episodes = 50000\n",
    "dataset = []\n",
    "createDataset(trajectory, agent, opponent, num_episodes)\n",
    "agentName = agent.__class__.__name__\n",
    "oppName = opponent.__class__.__name__\n",
    "# write dataset into a file\n",
    "write_list(dataset, agentName + str(depth) + \"Vs\" + oppName + \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1a520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDNN",
   "language": "python",
   "name": "pdnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
